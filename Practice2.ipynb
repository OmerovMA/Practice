{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom scipy.special import expit\n\n# Generate a toy dataset, it's just a straight line with some Gaussian noise:\nxmin, xmax = -5, 5\nn_samples = 100\nnp.random.seed(0)\nX = np.random.normal(size=n_samples)\ny = (X > 0).astype(float)\nX[X > 0] *= 4\nX += 0.3 * np.random.normal(size=n_samples)\n\nX = X[:, np.newaxis]\n\n# Fit the classifier\nclf = LogisticRegression(C=1e5)\nclf.fit(X, y)\n\n# and plot the result\nplt.figure(1, figsize=(4, 3))\nplt.clf()\nplt.scatter(X.ravel(), y, color=\"black\", zorder=20)\nX_test = np.linspace(-5, 10, 300)\n\nloss = expit(X_test * clf.coef_ + clf.intercept_).ravel()\nplt.plot(X_test, loss, color=\"red\", linewidth=3)\n\nols = LinearRegression()\nols.fit(X, y)\nplt.plot(X_test, ols.coef_ * X_test + ols.intercept_, linewidth=1)\nplt.axhline(0.5, color=\".5\")\n\nplt.ylabel(\"y\")\nplt.xlabel(\"X\")\nplt.xticks(range(-5, 10))\nplt.yticks([0, 0.5, 1])\nplt.ylim(-0.25, 1.25)\nplt.xlim(-4, 10)\nplt.legend(\n    (\"Logistic Regression Model\", \"Linear Regression Model\"),\n    loc=\"lower right\",\n    fontsize=\"small\",\n)\nplt.tight_layout()\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}